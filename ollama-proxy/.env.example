# Ollama API Proxy Configuration Example
# Copy this file to .env and modify as needed

# Ollama Configuration
OLLAMA_MODEL=qwen2.5-coder:7b
OLLAMA_ENDPOINT=http://localhost:11434
PROXY_PORT=8000

# Anthropic API Version (for compatibility)
ANTHROPIC_API_VERSION=2023-06-01

# Optional: Enable debug logging (true/false)
DEBUG=false

# Context Caching Configuration
CACHE_ENABLED=true
CACHE_DIR=./cache
MAX_ACTIVE_TOKENS=8000          # Trigger archival at this token count
MAX_TOTAL_TOKENS=100000         # Maximum total tokens (active + archived)
SUMMARY_RATIO=0.2               # Summarize to 20% of original size
CACHE_CLEANUP_DAYS=30           # Clean sessions older than this

# Session Management
AUTO_SESSION=true               # Automatically create sessions

# Smart Context Retrieval
SMART_RETRIEVAL=true            # Enable intelligent context loading
RETRIEVAL_THRESHOLD=0.6         # Relevance threshold (0.0-1.0)

# Alternative Models (uncomment to use):
# OLLAMA_MODEL=llama3.1:8b
# OLLAMA_MODEL=qwen2.5-coder:14b
# OLLAMA_MODEL=deepseek-r1:14b
# OLLAMA_MODEL=llava:13b  # For vision support

# Remote Ollama (if running on another machine):
# OLLAMA_ENDPOINT=http://192.168.1.100:11434

# Different Proxy Port (if 8000 is in use):
# PROXY_PORT=9000
